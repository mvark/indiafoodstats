name: Fetch India Products on OFF from Hugging Face

on:
  workflow_dispatch:  # manual trigger
jobs:
  fetch-and-save:
    runs-on: ubuntu-latest
    permissions:
      contents: write  # allow push to repo
    steps:
      - name: Checkout repo
        uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.x"
      
      - name: Install dependencies
        run: pip install pandas duckdb
      
      - name: Fetch data and save CSV
        run: |
          python << 'EOF'
          import os
          import pandas as pd
          from datetime import datetime, timezone
          import duckdb
          
          # --- Config ---
          PARQUET_URL = "https://huggingface.co/datasets/openfoodfacts/product-database/resolve/main/food.parquet"
          DATE_STR = datetime.now(timezone.utc).strftime("%Y-%m-%d")
          README_PATH = "data/readme.md"
          
          print("Querying Parquet file directly from URL...")
          
          # --- Query Parquet file directly using DuckDB ---
          try:
              # Create DuckDB connection
              con = duckdb.connect(database=':memory:')
              
              # Query to filter India products directly from remote Parquet
              query = f"""
              SELECT 
                  code,
                  product_name[1]['text'] as product_name,
                  brands,
                  categories,
                  nova_group,
                  nutriscore_grade,
                  link as url,
                  last_modified_t,
                  ingredients_text[1]['text'] as ingredients_text,
                  list_filter(nutriments, x -> x['name'] = 'energy-kcal')[1]['100g'] as "energy-kcal_100g",
                  list_filter(nutriments, x -> x['name'] = 'carbohydrates')[1]['100g'] as carbohydrates_100g,
                  list_filter(nutriments, x -> x['name'] = 'sugars')[1]['100g'] as sugars_100g,
                  list_filter(nutriments, x -> x['name'] = 'fiber')[1]['100g'] as fiber_100g,
                  list_filter(nutriments, x -> x['name'] = 'proteins')[1]['100g'] as proteins_100g,
                  list_filter(nutriments, x -> x['name'] = 'fat')[1]['100g'] as fat_100g,
                  list_filter(nutriments, x -> x['name'] = 'saturated-fat')[1]['100g'] as "saturated-fat_100g",
                  list_filter(nutriments, x -> x['name'] = 'salt')[1]['100g'] as salt_100g
              FROM read_parquet('{PARQUET_URL}')
              WHERE 
                  code IS NOT NULL 
                  AND code != ''
                  AND product_name IS NOT NULL 
                  AND len(product_name) > 0
                  AND list_contains(
                      list_transform(countries_tags, x -> lower(CAST(x AS VARCHAR))),
                      'en:india'
                  )
              """
              
              print("Executing query (this may take a few minutes)...")
              df = con.execute(query).df()
              con.close()
              
              print(f"Found {len(df)} India products")
              
          except Exception as e:
              print(f"Error querying Parquet file: {e}")
              import traceback
              traceback.print_exc()
              exit(1)
          
          # --- Convert last_modified_t to datetime ---
          if 'last_modified_t' in df.columns:
              df['last_modified_datetime'] = pd.to_datetime(df['last_modified_t'], unit='s', errors='coerce')
              df = df.drop('last_modified_t', axis=1)
          
          # --- Save CSV ---
          if df.empty:
              print("No records found. Exiting without saving.")
              exit(0)
          
          record_count = len(df)
          csv_filename = f"all_{DATE_STR}_{record_count}.csv"
          csv_path = os.path.join("data", csv_filename)
          os.makedirs("data", exist_ok=True)
          
          df.to_csv(csv_path, index=False)
          print(f"Saved {record_count} records to {csv_path}")
          
          # --- Update README.md with a markdown table ---
          note = f"| {DATE_STR} | {record_count} | `{csv_filename}` |\n"
          
          if os.path.exists(README_PATH):
              with open(README_PATH, "a", encoding="utf-8") as f:
                  f.write(note)
          else:
              with open(README_PATH, "w", encoding="utf-8") as f:
                  f.write("# Data Log\n\n")
                  f.write("| Date | Record Count | File |\n")
                  f.write("|------|--------------|------|\n")
                  f.write(note)
          
          print(f"Updated {README_PATH}")
          EOF
      
      - name: Commit and push changes
        run: |
          git config user.name "GitHub Actions Bot"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add data/
          git commit -m "Fetched all India products on $(date -u +'%Y-%m-%d')" || echo "No changes"
          git push
